{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a839dce1-b719-4417-a3fc-74cfe0f6d4c3",
   "metadata": {},
   "source": [
    "## similar to whats in folder 01-intro, just recreating and using only the random forest tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb3d9a7f-5658-4280-b5bd-d25c90d1f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from io import StringIO \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d3c301-1ed9-4864-814e-cad011e7f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a folder to store all .py files\n",
    "## Set up folders and paths\n",
    "\n",
    "source_folder = Path(\"src\")\n",
    "source_folder.mkdir(exist_ok=True)\n",
    "(source_folder / \"__init__.py\").touch()\n",
    "(source_folder / \"components\").mkdir(parents=True, exist_ok=True)\n",
    "(source_folder / \"components\" / \"__init__.py\").touch()\n",
    "\n",
    "sys.path.append(str(source_folder.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e22708-5234-44b0-bcaf-af52d357a300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/logger.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/logger.py\n",
    "# Create logger.py\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_FILE = f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\"\n",
    "logs_dir = os.path.join(os.getcwd(), \"logs\")\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "LOG_FILE_PATH = os.path.join(logs_dir, LOG_FILE)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE_PATH,\n",
    "    format=\"[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94ccc78-3369-491e-a274-b2bb6bd716c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path = \"data_schema/schema_yaml\"\n",
    "schema_dir = os.path.dirname(schema_path)\n",
    "\n",
    "os.makedirs(schema_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d21fa6-f1e7-463e-9747-a3060003d1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_schema/schema.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_schema/schema.yaml\n",
    "\n",
    "##write schema for the data type to compare later\n",
    "\n",
    "\n",
    "columns:\n",
    "- lpep_pickup_datetime: datetime64[us]\n",
    "- lpep_dropoff_datetime: datetime64[us]\n",
    "- PULocationID: object \n",
    "- DOLocationID: object \n",
    "- trip_distance: float64\n",
    "- passenger_count: float64\n",
    "- fare_amount: float64\n",
    "- total_amount: float64\n",
    "\n",
    "numerical_columns:\n",
    "- trip_distance: float64 \n",
    "- passenger_count: float64\n",
    "- fare_amount: float64\n",
    "- total_amount: float64\n",
    "\n",
    "\n",
    "categorical_columns:\n",
    "- DOLocationID: object \n",
    "- PULocationID: object\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f36b3-7e89-47d5-a796-686f06d30658",
   "metadata": {},
   "source": [
    "## DATA INGESTION AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65937bd-0fde-4a02-9a0e-41ae8fd798a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(source_folder / \"constants\").mkdir(parents=True, exist_ok=True)\n",
    "(source_folder / \"constants\" / \"__init__.py\").touch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ddff24c-6a4d-456d-aa2a-368276a44736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/constants/training_pipeline_names.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/constants/training_pipeline_names.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Data ingestion related constants, such as the data link, and the downloaded folder path\n",
    "\"\"\"\n",
    "\n",
    "DATA_INGESTION_TRAIN_LINK: str = \"./data/green_tripdata_2025-01.parquet\"\n",
    "DATA_INGESTION_TEST_LINK: str = \"./data/green_tripdata_2025-02.parquet\" \n",
    "DATA_INGESTION_INGESTED_DIR: str = \"ingested_data\"\n",
    "SCHEMA_FILE_PATH = os.path.join(\"data_schema\", \"schema.yaml\")\n",
    "\n",
    "\"\"\"\n",
    "Data transformation features, the columns required\n",
    "\"\"\"\n",
    "numeric_features = ['trip_distance', 'passenger_count','fare_amount','total_amount']\n",
    "target_encoder_features = ['PULocationID', 'DOLocationID']\n",
    "TARGET_COLUMN = ['duration']\n",
    "\n",
    "\"\"\"\n",
    "Model Trainer related constant start with MODEL TRAINER VAR NAME\n",
    "\"\"\"\n",
    "MODEL_TRAINER_TRAINED_MODEL_NAME: str = \"duration_time_model.pkl\"\n",
    "MODEL_TRAINER_EXPECTED_SCORE: float = 0.7\n",
    "MODEL_TRAINER_OVER_FITTING_UNDER_FITTING_THRESHOLD: float = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e64a35c-5635-4796-b044-1c7ca8f4526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/constants/config_entity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/constants/config_entity.py\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "from src.constants import training_pipeline_names\n",
    "from datetime import datetime\n",
    "import os\n",
    "from src.constants import training_pipeline_names\n",
    "\n",
    "class TrainingPipelineConfig:\n",
    "    def __init__(self, timestamp=datetime.now()):\n",
    "        timestamp = timestamp.strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "        self.data_artifact_folder = training_pipeline_names.DATA_INGESTION_INGESTED_DIR\n",
    "        self.data_folder = os.path.join(self.data_artifact_folder, f\"untransformed_data-{timestamp}\")  \n",
    "        self.timestamp: str = timestamp\n",
    "\n",
    "class DataIngestionConfig:\n",
    "    def __init__(self, training_pipeline_config: TrainingPipelineConfig):\n",
    "        self.train_data_source: str = training_pipeline_names.DATA_INGESTION_TRAIN_LINK\n",
    "        self.test_data_source: str = training_pipeline_names.DATA_INGESTION_TEST_LINK\n",
    "        self.training_file_path: str = os.path.join(training_pipeline_config.data_folder, \"train_data.csv\") \n",
    "        self.test_file_path: str = os.path.join(training_pipeline_config.data_folder, \"test_data.csv\")  \n",
    "\n",
    "class DataTransformationConfig:\n",
    "    def __init__(self, training_pipeline_config: TrainingPipelineConfig):\n",
    "        timestamp = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "        self.timestamp: str = timestamp\n",
    "        self.transformed_folder = os.path.join(training_pipeline_config.data_artifact_folder, f'transformed_data-{timestamp}')\n",
    "        self.model_path = os.path.join(training_pipeline_config.data_artifact_folder, f'model-{timestamp}')\n",
    "        self.transformed_train_file_path: str = os.path.join(self.transformed_folder, \"train.npy\")\n",
    "        self.transformed_test_file_path: str = os.path.join(self.transformed_folder, \"test.npy\")\n",
    "        self.transformed_object_file_path: str = os.path.join(self.model_path, \"preprocessing.pkl\")\n",
    "\n",
    "class ModelTrainerConfig:\n",
    "    def __init__(self, training_pipeline_config: TrainingPipelineConfig):\n",
    "        timestamp = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "        self.timestamp: str = timestamp\n",
    "        self.model_trainer_dir: str = os.path.join(training_pipeline_config.data_artifact_folder, f'model-{timestamp}')\n",
    "        self.trained_model_file_path: str = os.path.join(\n",
    "            self.model_trainer_dir, training_pipeline_names.MODEL_TRAINER_TRAINED_MODEL_NAME\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbbd46ee-7673-4df8-9730-81b99c91b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/constants/artifact_entity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/constants/artifact_entity.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionArtifact:\n",
    "    train_file_path: str\n",
    "    test_file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationArtifact:\n",
    "    transformed_object_file_path: str\n",
    "    transformed_train_file_path: str\n",
    "    transformed_test_file_path: str\n",
    "\n",
    "@dataclass\n",
    "class RegressionMetricArtifact:\n",
    "    mean_absolute_error: float\n",
    "    root_mean_squared_error: float\n",
    "    r2_score: float\n",
    "    \n",
    "@dataclass\n",
    "class ModelTrainerArtifact:\n",
    "    trained_model_file_path: str\n",
    "    train_metric_artifact: RegressionMetricArtifact\n",
    "    test_metric_artifact: RegressionMetricArtifact\n",
    "    model_performance: str\n",
    "    over_fitting_under_fitting: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fc4a52d-a732-4f39-b8dc-dbd8a0ba43fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/components/data_ingestion.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/components/data_ingestion.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml  \n",
    "from dataclasses import dataclass \n",
    "from src.logger import logging\n",
    "from typing import List\n",
    "from src.constants.config_entity import DataIngestionConfig, TrainingPipelineConfig\n",
    "from src.constants.artifact_entity import DataIngestionArtifact\n",
    "from src.constants import training_pipeline_names  \n",
    "\n",
    "\"\"\"\n",
    "This class handles:\n",
    "- assume to read train/test from an external source \n",
    "- remove unwanted columns\n",
    "- Validating column consistency\n",
    "- saves the data to my machine\n",
    "\"\"\"\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, data_ingestion_config: DataIngestionConfig):\n",
    "        try:\n",
    "            self.data_ingestion_config = data_ingestion_config\n",
    "            self._schema_config = self.read_yaml_file(training_pipeline_names.SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error initializing DataIngestion: {e}\") \n",
    "            \n",
    "    @staticmethod\n",
    "    def read_yaml_file(file_path: str) -> dict:\n",
    "        \"\"\"\n",
    "        Static method to read YAML configuration files\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"YAML file not found: {file_path}\")\n",
    "                \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as yaml_file:  \n",
    "                content = yaml.safe_load(yaml_file)\n",
    "                if content is None:\n",
    "                    raise ValueError(f\"YAML file is empty or invalid: {file_path}\")\n",
    "                return content\n",
    "        except yaml.YAMLError as e:\n",
    "            raise Exception(f\"Error parsing YAML file {file_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error reading YAML file {file_path}: {e}\")\n",
    "    \n",
    "    def read_external_dataframe(self):\n",
    "        \"\"\"\n",
    "        Read data from any data source and apply transformations\n",
    "        \"\"\"\n",
    "        try:\n",
    "            train_data_link = self.data_ingestion_config.train_data_source\n",
    "            test_data_link = self.data_ingestion_config.test_data_source\n",
    "            logging.info('Reading train and test data sources')\n",
    "            \n",
    "            # Check if files exist before reading\n",
    "            if not os.path.exists(train_data_link):\n",
    "                raise FileNotFoundError(f\"Train data file not found: {train_data_link}\")\n",
    "            if not os.path.exists(test_data_link):\n",
    "                raise FileNotFoundError(f\"Test data file not found: {test_data_link}\")\n",
    "            \n",
    "            # Read parquet files\n",
    "            train_data = pd.read_parquet(train_data_link)\n",
    "            test_data = pd.read_parquet(test_data_link)\n",
    "            \n",
    "            logging.info(f\"Loaded {len(train_data)} training and {len(test_data)} test records\")\n",
    "            print(f\"Loaded {len(train_data)} training and {len(test_data)} test records from database\")\n",
    "      \n",
    "            # Define columns to drop (taxi-specific columns)\n",
    "            columns_to_drop = [\n",
    "                'VendorID', 'store_and_fwd_flag', 'RatecodeID', \n",
    "                'extra', 'mta_tax', 'tip_amount', 'tolls_amount', \n",
    "                'ehail_fee', 'improvement_surcharge', 'payment_type', \n",
    "                'trip_type', 'congestion_surcharge', 'cbd_congestion_fee'\n",
    "            ]\n",
    "            \n",
    "            # Check if columns exist before dropping\n",
    "            existing_cols_train = [col for col in columns_to_drop if col in train_data.columns]\n",
    "            if existing_cols_train:\n",
    "                train_data = train_data.drop(columns=existing_cols_train, axis=1)\n",
    "                logging.info(f\"Dropped {len(existing_cols_train)} columns from training data\")\n",
    "            \n",
    "            existing_cols_test = [col for col in columns_to_drop if col in test_data.columns]\n",
    "            if existing_cols_test:\n",
    "                test_data = test_data.drop(columns=existing_cols_test, axis=1)\n",
    "                logging.info(f\"Dropped {len(existing_cols_test)} columns from test data\")\n",
    "                \n",
    "            # Convert all \"na\" strings to numpy NaN\n",
    "            train_data.replace({\"na\": np.nan}, inplace=True)\n",
    "            test_data.replace({\"na\": np.nan}, inplace=True)\n",
    "            \n",
    "            logging.info(\"Data transformation completed successfully\")\n",
    "            return train_data, test_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in reading and transforming dataframe: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def validate_column(self, train_data: pd.DataFrame, test_data: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Validate that the required columns are present in both datasets\n",
    "        \"\"\"\n",
    "        try:\n",
    "            required_columns = []  # Fixed variable name\n",
    "            \n",
    "            # Extract required columns from schema config\n",
    "            if \"columns\" in self._schema_config:\n",
    "                for column_dict in self._schema_config[\"columns\"]:\n",
    "                    if isinstance(column_dict, dict):\n",
    "                        required_columns.extend(column_dict.keys())\n",
    "                    elif isinstance(column_dict, str):\n",
    "                        required_columns.append(column_dict)\n",
    "            else:\n",
    "                logging.warning(\"No 'columns' key found in schema config\")\n",
    "                return True  # Skip validation if no schema defined\n",
    "            \n",
    "            logging.info(f\"Required columns: {required_columns}\")\n",
    "            logging.info(f\"Train columns: {list(train_data.columns)}\")\n",
    "            logging.info(f\"Test columns: {list(test_data.columns)}\")\n",
    "            \n",
    "            # Validate train data columns\n",
    "            train_validation = set(required_columns) == set(train_data.columns)\n",
    "            if train_validation:\n",
    "                logging.info(\"Train Column validation passed.\")\n",
    "            else:\n",
    "                logging.error(\"Train Column validation failed.\")\n",
    "                missing_train = set(required_columns) - set(train_data.columns)\n",
    "                extra_train = set(train_data.columns) - set(required_columns)\n",
    "                if missing_train:\n",
    "                    logging.error(f\"Missing columns in train data: {missing_train}\")\n",
    "                if extra_train:\n",
    "                    logging.error(f\"Extra columns in train data: {extra_train}\")\n",
    "            \n",
    "            # Validate test data columns\n",
    "            test_validation = set(required_columns) == set(test_data.columns)\n",
    "            if test_validation:\n",
    "                logging.info(\"Test Column validation passed.\")\n",
    "            else:\n",
    "                logging.error(\"Test Column validation failed.\")\n",
    "                missing_test = set(required_columns) - set(test_data.columns)\n",
    "                extra_test = set(test_data.columns) - set(required_columns)\n",
    "                if missing_test:\n",
    "                    logging.error(f\"Missing columns in test data: {missing_test}\")\n",
    "                if extra_test:\n",
    "                    logging.error(f\"Extra columns in test data: {extra_test}\")\n",
    "            \n",
    "            # Return True only if both validations pass\n",
    "            return train_validation and test_validation\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error validating columns: {e}\")\n",
    "            raise e\n",
    "            \n",
    "    def save_data_to_machine(self, train_data: pd.DataFrame, test_data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Save the processed data to local machine\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            data_path = os.path.dirname(self.data_ingestion_config.training_file_path)\n",
    "            os.makedirs(data_path, exist_ok=True)\n",
    "            \n",
    "            logging.info(f\"Created directory: {data_path}\")\n",
    "            \n",
    "            # Save dataframes as CSV files\n",
    "            train_data.to_csv(self.data_ingestion_config.training_file_path, index=False, header=True)\n",
    "            test_data.to_csv(self.data_ingestion_config.test_file_path, index=False, header=True)\n",
    "            \n",
    "            logging.info(\"Dataframes successfully saved to local machine\")\n",
    "            \n",
    "            return self.data_ingestion_config.training_file_path, self.data_ingestion_config.test_file_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in saving data to machine: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def start_data_ingestion(self):\n",
    "        \"\"\"\n",
    "        Main method to start the data ingestion process\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Starting data ingestion process\")\n",
    "            \n",
    "            # Read and transform data\n",
    "            train_data, test_data = self.read_external_dataframe()\n",
    "\n",
    "            # Validate columns\n",
    "            validation_result = self.validate_column(train_data, test_data)\n",
    "            if not validation_result:\n",
    "                raise Exception(\"Column validation failed. Check logs for details.\")\n",
    "            \n",
    "            # Save data to machine\n",
    "            train_file_path, test_file_path = self.save_data_to_machine(train_data, test_data)\n",
    "            \n",
    "            # Create artifact\n",
    "            data_ingestion_artifact = DataIngestionArtifact(\n",
    "                train_file_path=self.data_ingestion_config.training_file_path,\n",
    "                test_file_path=self.data_ingestion_config.test_file_path\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Data ingestion completed successfully\")\n",
    "            return data_ingestion_artifact\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in data ingestion process: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae37c7-b228-4a7c-904f-3f4e1e1efc8e",
   "metadata": {},
   "source": [
    "### DATA TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86eed3c3-3e8f-4559-8da9-0ebb5c605264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/components/data_transformation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/components/data_transformation.py\n",
    "\n",
    "# src/components/data_transformation.py\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import TargetEncoder\n",
    "from src.constants.training_pipeline_names import ( \n",
    "    numeric_features,\n",
    "    target_encoder_features,\n",
    "    TARGET_COLUMN\n",
    ")\n",
    "from src.constants.config_entity import (\n",
    "    DataIngestionConfig,\n",
    "    TrainingPipelineConfig,\n",
    "    DataTransformationConfig\n",
    ")\n",
    "from src.constants.artifact_entity import DataIngestionArtifact, DataTransformationArtifact\n",
    "from src.logger import logging\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, data_ingestion_artifact: DataIngestionArtifact,\n",
    "                 data_transformation_config: DataTransformationConfig):\n",
    "        try:\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_transformation_config = data_transformation_config\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_data(filepath) -> pd.DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(filepath)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_numpy_data(file_path: str, array: np.array):\n",
    "        \"\"\"\n",
    "        Save numpy array data to file\n",
    "        file_path: str location of file to save\n",
    "        array: np.array data to save\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dir_path = os.path.dirname(file_path)\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            with open(file_path, \"wb\") as file_obj:\n",
    "                np.save(file_obj, array)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_object(file_path: str, obj: object) -> None:\n",
    "        \"\"\"\n",
    "        this static method saves the preprocessor pkl\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Entered the save_object method class\")\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            with open(file_path, \"wb\") as file_obj:\n",
    "                joblib.dump(obj, file_obj)\n",
    "            logging.info(\"Exited the save_object method class\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    def create_data_transformer(self):\n",
    "        \"\"\"\n",
    "        This function creates and returns the preprocessing object with imputation\n",
    "        \"\"\"\n",
    "        logging.info(\"creating encoders and imputers\")\n",
    "        try:\n",
    "            # Create preprocessing pipelines for each feature type\n",
    "            numeric_pipeline = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ])\n",
    "            \n",
    "            categorical_pipeline = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('target_encoder', TargetEncoder())\n",
    "            ])\n",
    "            \n",
    "            logging.info(\"Initiating encoders and scaling\")\n",
    "            # Create preprocessor using ColumnTransformer\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('numerical_transformer', numeric_pipeline, numeric_features),\n",
    "                    ('categorical_transformer', categorical_pipeline, target_encoder_features)\n",
    "                ],\n",
    "                remainder='drop'\n",
    "            )\n",
    "            return preprocessor\n",
    "        except Exception as e:\n",
    "            logging.error(f'Could not transform the inputs: {e}')\n",
    "            raise Exception(f\"Error in creating preprocessor: {e}\")\n",
    "    \n",
    "    def create_processed_data(self, untransformed_data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Load and preprocess data from DataFrame\n",
    "        \n",
    "        creates the target column\n",
    "        \n",
    "        Returns:\n",
    "        processed dataframe with target column and selected features\n",
    "        \"\"\"\n",
    "        # Calculate trip duration in minutes\n",
    "        untransformed_data['lpep_pickup_datetime'] = pd.to_datetime(untransformed_data['lpep_pickup_datetime'])\n",
    "        untransformed_data['lpep_dropoff_datetime'] = pd.to_datetime(untransformed_data['lpep_dropoff_datetime'])\n",
    "        \n",
    "        untransformed_data['duration'] = (\n",
    "            untransformed_data['lpep_dropoff_datetime'] - untransformed_data['lpep_pickup_datetime']\n",
    "        ).dt.total_seconds() / 60\n",
    "        \n",
    "        # Select final columns\n",
    "        data = untransformed_data[['PULocationID', 'DOLocationID', 'passenger_count','trip_distance','fare_amount','total_amount','duration']]\n",
    "        \t\n",
    "        \n",
    "        # Remove outliers - filter duration and distance, duration more than one hour and 20 miles\n",
    "        data = data[(data['duration'] >= 0) & (data['duration'] <= 60)]\n",
    "        data = data[(data['trip_distance'] >= 0) & (data['trip_distance'] <= 15)]\n",
    "        data = data[(data['passenger_count'] >= 0) & (data['passenger_count'] <= 5)]\n",
    "        \n",
    "        # Convert location IDs to categorical data\n",
    "        data[['PULocationID', 'DOLocationID']] = (\n",
    "            data[['PULocationID', 'DOLocationID']].astype('str')\n",
    "        )\n",
    "        return data\n",
    "        \n",
    "    def start_data_transformation(self):\n",
    "        \"\"\"\n",
    "        This method initiates the data transformation with imputation\n",
    "        \"\"\"\n",
    "        logging.info(\"Starting data transformation\")\n",
    "        try:\n",
    "            # Read training and testing data via the static method function\n",
    "            train_df = DataTransformation.read_data(self.data_ingestion_artifact.train_file_path)\n",
    "            test_df = DataTransformation.read_data(self.data_ingestion_artifact.test_file_path)\n",
    "            logging.info(\"Read both test and train data successfully\")\n",
    "            \n",
    "            # Check for missing values before transformation\n",
    "            logging.info(f\"Train data missing values:\\n{train_df.isnull().sum()}\")\n",
    "            logging.info(f\"Test data missing values:\\n{test_df.isnull().sum()}\")\n",
    "            \n",
    "            train_df = self.create_processed_data(train_df)\n",
    "            test_df = self.create_processed_data(test_df)\n",
    "            logging.info(\"created the duration target column\")\n",
    "            \n",
    "            # Separate features and target variable\n",
    "            input_feature_train_df = train_df.drop(columns=TARGET_COLUMN, axis=1)\n",
    "            target_feature_train_df = train_df[TARGET_COLUMN[0]]  # TARGET_COLUMN is a list\n",
    "            logging.info(\"Split the train data into features and target\")\n",
    "            \n",
    "            input_feature_test_df = test_df.drop(columns=TARGET_COLUMN, axis=1)\n",
    "            target_feature_test_df = test_df[TARGET_COLUMN[0]]  # TARGET_COLUMN is a list\n",
    "            logging.info(\"Split the test data into features and target\")\n",
    "            \n",
    "            # Get the preprocessor object\n",
    "            preprocessor = self.create_data_transformer()\n",
    "            \n",
    "            # Apply transformations\n",
    "            logging.info(\"Applying transformations on training data\")\n",
    "            transformed_input_train_feature = preprocessor.fit_transform(input_feature_train_df, target_feature_train_df)\n",
    "            logging.info(\"Applying transformations on test data\")\n",
    "            transformed_input_test_feature = preprocessor.transform(input_feature_test_df)\n",
    "            \n",
    "            # Combine transformed features with target variable\n",
    "            logging.info(\"concatenate the transformed array\")\n",
    "            train_arr = np.c_[\n",
    "                transformed_input_train_feature, np.array(target_feature_train_df)\n",
    "            ]\n",
    "            test_arr = np.c_[\n",
    "                transformed_input_test_feature, np.array(target_feature_test_df)\n",
    "            ]\n",
    "            \n",
    "            logging.info(\"Transformation completed successfully\")\n",
    "            logging.info(f\"Transformed train array shape: {train_arr.shape}\")\n",
    "            logging.info(f\"Transformed test array shape: {test_arr.shape}\")\n",
    "            \n",
    "            # Save the train, test and transformation file\n",
    "            logging.info(\"saving the train, test and transformer\")\n",
    "            DataTransformation.save_numpy_data(self.data_transformation_config.transformed_train_file_path, array=train_arr)\n",
    "            DataTransformation.save_numpy_data(self.data_transformation_config.transformed_test_file_path, array=test_arr)\n",
    "            DataTransformation.save_object(self.data_transformation_config.transformed_object_file_path, preprocessor)\n",
    "            \n",
    "            # Prepare data transformation artifact\n",
    "            data_transformation_artifact = DataTransformationArtifact(\n",
    "                transformed_object_file_path=self.data_transformation_config.transformed_object_file_path,\n",
    "                transformed_train_file_path=self.data_transformation_config.transformed_train_file_path,\n",
    "                transformed_test_file_path=self.data_transformation_config.transformed_test_file_path,\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Data transformation artifact: {data_transformation_artifact}\")\n",
    "            return data_transformation_artifact\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in data transformation: {e}\")\n",
    "            raise Exception(f\"Data transformation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15722f-bdcb-4c09-9ecf-c0b899760c25",
   "metadata": {},
   "source": [
    "### Model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0834fd5-5308-40fe-8227-9502aaccee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/components/ModelTrainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/components/ModelTrainer.py\n",
    "#MLflow setup:\n",
    "#MLflow setup:\n",
    "#- tracking server: yes, local server\n",
    "#- backend store: sqlite database, this houses meta data, metrics, params\n",
    "#- artifacts store: local filesystem\n",
    "#To run this example you need to launch the mlflow server locally by running the following command in your terminal:\n",
    "#lsof -ti :5000 | xargs kill -9 lsof -ti :8000 | xargs kill -9\n",
    "\n",
    "##run this in terminal but in the folder \n",
    "\n",
    "#mlflow server --backend-store-uri sqlite:///mlflow.db --host 127.0.0.1 --port 5000\n",
    "\n",
    "#export MLFLOW_TRACKING_URI=\"http://127.0.0.1:5000\"\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import joblib\n",
    "from src.constants.training_pipeline_names import MODEL_TRAINER_EXPECTED_SCORE, MODEL_TRAINER_OVER_FITTING_UNDER_FITTING_THRESHOLD\n",
    "from src.constants.config_entity import (\n",
    "    DataTransformationConfig,\n",
    "    ModelTrainerConfig\n",
    ")\n",
    "from src.constants.artifact_entity import (\n",
    "    DataTransformationArtifact,\n",
    "    ModelTrainerArtifact,\n",
    "    RegressionMetricArtifact\n",
    ")\n",
    "from src.logger import logging\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "\n",
    "def load_object(file_path: str):\n",
    "    \"\"\"Load object from file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file_obj:\n",
    "            return joblib.load(file_obj)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def load_numpy_array_data(file_path: str) -> np.array:\n",
    "    \"\"\"Load numpy array data from file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file_obj:\n",
    "            return np.load(file_obj)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model_trainer_config: ModelTrainerConfig, \n",
    "                 data_transformation_artifact: DataTransformationArtifact):\n",
    "        \n",
    "        try:\n",
    "            mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "            mlflow.set_tag('development','duration model')\n",
    "            logging.info(\"Starting ModelTrainer initialization and setting up mlflow\")\n",
    "            print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")\n",
    "            logging.info(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")\n",
    "            \n",
    "            self.model_trainer_config = model_trainer_config\n",
    "            self.data_transformation_artifact = data_transformation_artifact\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in ModelTrainer initialization: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def _ensure_mlflow_run_ended(self):\n",
    "        \"\"\"Safely end any active MLflow run\"\"\"\n",
    "        try:\n",
    "            if mlflow.active_run():\n",
    "                logging.info(\"Active MLflow run detected, ending it safely\")\n",
    "                mlflow.end_run()\n",
    "                logging.info(\"Previous MLflow run ended successfully\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error ending MLflow run: {e}\")\n",
    "            # Continue execution as this is not critical\n",
    "\n",
    "    def _save_model_safely(self, model, model_dir_path: str, model_filename: str = 'my_model.ubj'):\n",
    "        \"\"\"\n",
    "        Safely save XGBoost model with proper error handling and path management\n",
    "        \n",
    "        Args:\n",
    "            model: Trained XGBoost model\n",
    "            model_dir_path: Base directory path from config\n",
    "            model_filename: Name of the model file (default: 'my_model.ubj')\n",
    "            \n",
    "        Returns:\n",
    "            str: Full path where model was saved\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Option 3: Most robust - ensure directory exists and create full path\n",
    "            model_dir = os.path.dirname(model_dir_path)\n",
    "            \n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            logging.info(f\"Model directory created/verified: {model_dir}\")\n",
    "            \n",
    "            # Create full model path\n",
    "            model_path = os.path.join(model_dir, model_filename)\n",
    "            \n",
    "            # Validate path before saving\n",
    "            if not os.path.exists(model_dir):\n",
    "                raise FileNotFoundError(f\"Model directory does not exist: {model_dir}\")\n",
    "                \n",
    "            # Save the model with error handling\n",
    "            logging.info(f\"Attempting to save model to: {model_path}\")\n",
    "            \n",
    "            try:\n",
    "                model.save_model(model_path)\n",
    "                logging.info(f\"Model saved successfully to: {model_path}\")\n",
    "                \n",
    "                # Verify the file was actually created\n",
    "                if os.path.exists(model_path):\n",
    "                    file_size = os.path.getsize(model_path)\n",
    "                    logging.info(f\"Model file verified - Size: {file_size} bytes\")\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"Model file was not created: {model_path}\")\n",
    "                    \n",
    "                return model_path\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to save model: {e}\")\n",
    "                raise e\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in _save_model_safely: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def train_model(self, X_train, y_train, X_test, y_test):\n",
    "        \n",
    "        try:\n",
    "            # Safely end any active MLflow run\n",
    "            self._ensure_mlflow_run_ended()\n",
    "            \n",
    "            mlflow.set_experiment(\"new-york-taxi-drive-duration-train-January\")\n",
    "            with mlflow.start_run() as run:\n",
    "                results = [] \n",
    "                logging.info(\"Starting model training process\")\n",
    "                logging.info(f\"Training data shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "                logging.info(f\"Test data shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "    \n",
    "                logging.info(\"Loading the Xgboost model\")\n",
    "                \n",
    "                # Train model\n",
    "                model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "                model.fit(X_train, y_train)\n",
    "                logging.info(\"Making predictions with the model\")\n",
    "                \n",
    "                # Make predictions\n",
    "                y_train_pred = model.predict(X_train)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                logging.info(\"Calculating regression metrics\")\n",
    "                train_mae, train_rmse, train_r2 = evaluate_model(y_train, y_train_pred)\n",
    "                test_mae, test_rmse, test_r2 = evaluate_model(y_test, y_test_pred)\n",
    "                \n",
    "                ##signature for mlflow\n",
    "                signature = infer_signature(X_test, y_test_pred)\n",
    "                \n",
    "                # Create metric artifacts\n",
    "                regression_train_metric = RegressionMetricArtifact(\n",
    "                    mean_absolute_error=train_mae,\n",
    "                    root_mean_squared_error=train_rmse,\n",
    "                    r2_score=train_r2\n",
    "                )\n",
    "                \n",
    "                regression_test_metric = RegressionMetricArtifact(\n",
    "                    mean_absolute_error=test_mae,\n",
    "                    root_mean_squared_error=test_rmse,\n",
    "                    r2_score=test_r2\n",
    "                )\n",
    "                \n",
    "                # Append results\n",
    "                results.append({\n",
    "                    'Model': \"XGBOOST\",\n",
    "                    'Train_MAE': train_mae,\n",
    "                    'Train_RMSE': train_rmse,\n",
    "                    'Train_R2': train_r2,\n",
    "                    'Test_MAE': test_mae,\n",
    "                    'Test_RMSE': test_rmse,\n",
    "                    'Test_R2': test_r2,\n",
    "                    'Overfit_Check': train_r2 - test_r2  \n",
    "                })\n",
    "    \n",
    "                logging.info(f\"Training metrics: {train_mae, train_rmse, train_r2}\")\n",
    "                logging.info(f\"Test metrics: {test_mae, test_rmse, test_r2}\")\n",
    "                logging.info(\"logging MLflow metrics and params\")\n",
    "                \n",
    "                params = {\n",
    "                    \"model_name\": \"XGBOOST\", \n",
    "                    \"training_data_shape\": str(X_train.shape),\n",
    "                    \"test_data_shape\": str(X_test.shape), \n",
    "                    \"n_estimators\": 100, \n",
    "                    \"max_depth\": 3, \n",
    "                    \"learning_rate\": 0.1\n",
    "                }\n",
    "                mlflow.log_params(params)\n",
    "            \n",
    "                mlflow.log_metrics({\n",
    "                    'Train_MAE': train_mae,\n",
    "                    'Train_RMSE': train_rmse,\n",
    "                    'Train_R2': train_r2,\n",
    "                    'Test_MAE': test_mae,\n",
    "                    'Test_RMSE': test_rmse,\n",
    "                    'Test_R2': test_r2,\n",
    "                    'Overfit_Check': train_r2 - test_r2\n",
    "                })\n",
    "\n",
    "                # log and save model\n",
    "                model_name = \"nyc-taxi-duration-model-January\"\n",
    "                registered_model_name=\"XGBoostdurationModel\"\n",
    "                \n",
    "                \n",
    "                if test_r2 > MODEL_TRAINER_EXPECTED_SCORE:\n",
    "                    try:\n",
    "                        mlflow.xgboost.log_model(\n",
    "                            xgb_model=model,\n",
    "                            name=model_name,\n",
    "                            registered_model_name=registered_model_name,\n",
    "                            input_example=X_test[:5],\n",
    "                            signature=signature,\n",
    "                            model_format=\"ubj\"\n",
    "                        )\n",
    "                        logging.info(f\"Model registered as: {registered_model_name}\")\n",
    "                        print(f\"Model registered as: {registered_model_name}\")\n",
    "                        print(f\"Run ID: {run.info.run_id}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Failed to register model in MLflow: {e}\")\n",
    "                        # Continue execution, just log the model without registration\n",
    "                        mlflow.xgboost.log_model(model, artifact_path=\"nyc-duration-model\")\n",
    "                        logging.info(\"Model logged without registration due to error\")\n",
    "                else:\n",
    "                    # Just log without registering if performance is poor\n",
    "                    #mlflow.xgboost.log_model(model, artifact_path=\"nyc-duration-model\")\n",
    "                    mlflow.xgboost.log_model(model, name=model_name)                        \n",
    "                    logging.info(\"Model not registered due to poor performance\")\n",
    "                    \n",
    "                # Load preprocessor to save to mlflow as artifact\n",
    "                try:\n",
    "                    preprocessor = load_object(file_path=self.data_transformation_artifact.transformed_object_file_path)\n",
    "                    logging.info(\"Preprocessor loaded successfully\")\n",
    "                    mlflow.log_artifact(self.data_transformation_artifact.transformed_object_file_path, artifact_path=\"preprocessor\")\n",
    "                    logging.info(\"Preprocessor logged to MLflow successfully\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to load or log preprocessor: {e}\")\n",
    "                    # Continue execution as this is not critical for model saving\n",
    "    \n",
    "                # Save the trained model using the robust method\n",
    "                logging.info(\"Starting model save process\")\n",
    "                try:\n",
    "                    saved_model_path = self._save_model_safely(\n",
    "                        model=model,\n",
    "                        model_dir_path=self.model_trainer_config.trained_model_file_path,\n",
    "                        model_filename='my_model.ubj'\n",
    "                    )\n",
    "                    logging.info(f\"Model saved successfully at: {saved_model_path}\")\n",
    "                    \n",
    "                    # Update the artifact path to the actual saved path\n",
    "                    actual_model_path = saved_model_path\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Critical error: Failed to save model: {e}\")\n",
    "                    raise e\n",
    "    \n",
    "                # Model performance evaluation\n",
    "                if results[-1]['Test_R2'] > MODEL_TRAINER_EXPECTED_SCORE:\n",
    "                    remark = \"good score\"\n",
    "                else:\n",
    "                    remark = \"bad\"\n",
    "    \n",
    "                # Create Model Trainer Artifact with the actual saved path\n",
    "                model_trainer_artifact = ModelTrainerArtifact(\n",
    "                    trained_model_file_path=actual_model_path,\n",
    "                    train_metric_artifact=regression_train_metric,\n",
    "                    test_metric_artifact=regression_test_metric,\n",
    "                    model_performance=remark,\n",
    "                    over_fitting_under_fitting=results[-1]['Overfit_Check']\n",
    "                )\n",
    "                \n",
    "                logging.info(f\"Model trainer artifact created: {model_trainer_artifact}\")\n",
    "                logging.info(\"Model training process completed successfully\")\n",
    "    \n",
    "                print(\"model file path:\")\n",
    "                print(model_trainer_artifact.trained_model_file_path)\n",
    "                print('---' * 20)\n",
    "                print(\"Training Metrics:\")\n",
    "                print(model_trainer_artifact.train_metric_artifact)\n",
    "                print('---' * 20)\n",
    "                print(\"Test Metrics:\")\n",
    "                print(model_trainer_artifact.test_metric_artifact)\n",
    "                print('---' * 20)\n",
    "                print(\"Over_fitting_under_fitting_Metrics:\")\n",
    "                print(model_trainer_artifact.over_fitting_under_fitting)\n",
    "                print('---' * 20)\n",
    "                print(\"Training performance:\")\n",
    "                print(model_trainer_artifact.model_performance)\n",
    "                \n",
    "                return model_trainer_artifact\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in train_model method: {e}\")\n",
    "            # Ensure MLflow run is ended even if there's an error\n",
    "            self._ensure_mlflow_run_ended()\n",
    "            raise e\n",
    "\n",
    "    def start_model_trainer(self) -> ModelTrainerArtifact:\n",
    "        \n",
    "        try:\n",
    "            logging.info(\"Starting model trainer pipeline\")\n",
    "            \n",
    "            train_file_path = self.data_transformation_artifact.transformed_train_file_path\n",
    "            test_file_path = self.data_transformation_artifact.transformed_test_file_path \n",
    "            \n",
    "            logging.info(f\"Loading training data from: {train_file_path}\")\n",
    "            logging.info(f\"Loading test data from: {test_file_path}\")\n",
    "            \n",
    "            # Loading training array and testing array\n",
    "            train_arr = load_numpy_array_data(train_file_path)\n",
    "            test_arr = load_numpy_array_data(test_file_path)\n",
    "            \n",
    "            logging.info(f\"Training array shape: {train_arr.shape}\")\n",
    "            logging.info(f\"Test array shape: {test_arr.shape}\")\n",
    "            \n",
    "            # Split features and target\n",
    "            x_train, y_train, x_test, y_test = (\n",
    "                train_arr[:, :-1],\n",
    "                train_arr[:, -1],\n",
    "                test_arr[:, :-1],\n",
    "                test_arr[:, -1],\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Data split completed - features and target separated\")\n",
    "            logging.info(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "            logging.info(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "            \n",
    "            # Train model with all required parameters\n",
    "            logging.info(\"Calling train_model method\")\n",
    "            model_trainer_artifact = self.train_model(x_train, y_train, x_test, y_test)\n",
    "            \n",
    "            logging.info(\"Model trainer pipeline completed successfully\")\n",
    "            return model_trainer_artifact\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in start_model_trainer method: {e}\")\n",
    "            # Ensure any active MLflow runs are ended\n",
    "            self._ensure_mlflow_run_ended()\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b4955c3-0d64-4f20-a704-88b131918bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "# main.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass \n",
    "from src.logger import logging\n",
    "from typing import List\n",
    "from src.constants.config_entity import (\n",
    "    DataIngestionConfig, \n",
    "    TrainingPipelineConfig, \n",
    "    DataTransformationConfig,\n",
    "    ModelTrainerConfig\n",
    ")\n",
    "from src.constants.artifact_entity import DataIngestionArtifact, DataTransformationArtifact\n",
    "from src.components.data_ingestion import DataIngestion\n",
    "from src.components.data_transformation import DataTransformation\n",
    "from src.components.ModelTrainer import ModelTrainer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize pipeline configuration\n",
    "        training_pipeline_config = TrainingPipelineConfig()\n",
    "        data_ingestion_config = DataIngestionConfig(training_pipeline_config)\n",
    "        \n",
    "        # Create data ingestion instance\n",
    "        data_ingestion = DataIngestion(data_ingestion_config)\n",
    "        \n",
    "        logging.info(\"Initiating data reading and processing\")\n",
    "        \n",
    "        # Start data ingestion\n",
    "        data_ingestion_artifact = data_ingestion.start_data_ingestion()\n",
    "        \n",
    "        print(\"Data ingestion completed successfully!\")\n",
    "        print(f\"Train file: {data_ingestion_artifact.train_file_path}\")\n",
    "        print(f\"Test file: {data_ingestion_artifact.test_file_path}\")\n",
    "        \n",
    "        logging.info(\"Transforming data\")\n",
    "        data_transformation_config = DataTransformationConfig(training_pipeline_config)\n",
    "        \n",
    "        # Create data transformation instance\n",
    "        data_transform = DataTransformation(data_ingestion_artifact, data_transformation_config)\n",
    "        data_transformation_artifact = data_transform.start_data_transformation()\n",
    "        \n",
    "        print(\"Data transformation completed successfully!\")\n",
    "        print(f\"Preprocessor file: {data_transformation_artifact.transformed_object_file_path}\")\n",
    "        print(f\"Transformed train file: {data_transformation_artifact.transformed_train_file_path}\")\n",
    "        print(f\"Transformed test file: {data_transformation_artifact.transformed_test_file_path}\")\n",
    "        logging.info(\"Transforming ended\")\n",
    "        \n",
    "        logging.info(\"model training\")\n",
    "        model_trainer_config = ModelTrainerConfig(training_pipeline_config)\n",
    "        # Create model training instance\n",
    "        model_trainer = ModelTrainer(model_trainer_config, data_transformation_artifact)\n",
    "        model_trainer_artifact = model_trainer.start_model_trainer()\n",
    "        \n",
    "        print(\"Model successfully trained!\")\n",
    "        logging.info(\"Model training ended\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1914969-c2dc-41b2-a640-983188ae8db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 48326 training and 46621 test records from database\n",
      "Data ingestion completed successfully!\n",
      "Train file: ingested_data/untransformed_data-09_13_2025_14_52_56/train_data.csv\n",
      "Test file: ingested_data/untransformed_data-09_13_2025_14_52_56/test_data.csv\n",
      "Data transformation completed successfully!\n",
      "Preprocessor file: ingested_data/model-09_13_2025_14_52_58/preprocessing.pkl\n",
      "Transformed train file: ingested_data/transformed_data-09_13_2025_14_52_58/train.npy\n",
      "Transformed test file: ingested_data/transformed_data-09_13_2025_14_52_58/test.npy\n",
      "tracking URI: 'http://127.0.0.1:5000'\n",
      "🏃 View run intrigued-doe-398 at: http://127.0.0.1:5000/#/experiments/0/runs/61298ae5f8dd49479548493c3ec1486c\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/0\n",
      "Registered model 'XGBoostdurationModel' already exists. Creating a new version of this model...\n",
      "2025/09/13 14:53:01 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: XGBoostdurationModel, version 2\n",
      "Created version '2' of model 'XGBoostdurationModel'.\n",
      "Model registered as: XGBoostdurationModel\n",
      "Run ID: fa58f114678a4fbcba489beb1395a12f\n",
      "model file path:\n",
      "ingested_data/model-09_13_2025_14_52_58/my_model.ubj\n",
      "------------------------------------------------------------\n",
      "Training Metrics:\n",
      "RegressionMetricArtifact(mean_absolute_error=1.7034411683872916, root_mean_squared_error=3.241462137169792, r2_score=0.8503334425703003)\n",
      "------------------------------------------------------------\n",
      "Test Metrics:\n",
      "RegressionMetricArtifact(mean_absolute_error=1.770204698150398, root_mean_squared_error=3.4969764455276646, r2_score=0.8269245094073274)\n",
      "------------------------------------------------------------\n",
      "Over_fitting_under_fitting_Metrics:\n",
      "0.023408933162972878\n",
      "------------------------------------------------------------\n",
      "Training performance:\n",
      "good score\n",
      "🏃 View run puzzled-turtle-900 at: http://127.0.0.1:5000/#/experiments/3/runs/fa58f114678a4fbcba489beb1395a12f\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/3\n",
      "Model successfully trained!\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c31630-cdb3-4d40-a548-07bc7e8a0948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f90ba6c8-908f-40a2-bd19-aa74fd837a30",
   "metadata": {},
   "source": [
    "### REQUIREMENT.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "138e02a1-68bc-4bcd-8724-069e25abce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "mlflow==3.3.2\n",
    "cloudpickle==3.0.0\n",
    "lz4==4.3.2\n",
    "numpy==1.26.4\n",
    "pandas==2.3.1\n",
    "psutil==7.0.0\n",
    "scikit-learn==1.7.0\n",
    "scipy==1.13.0\n",
    "fastapi==0.115.12\n",
    "uvicorn==0.34.2\n",
    "category-encoders==2.6.3\n",
    "jinja2==3.1.3\n",
    "joblib==1.4.2\n",
    "email-validator==2.1.1\n",
    "uvicorn[standard]==0.24.0\n",
    "python-multipart==0.0.6\n",
    "pandas==2.1.4\n",
    "pydantic==2.5.2\n",
    "xgboost==3.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253e9f2-6682-4203-bfc1-250228ac7b0d",
   "metadata": {},
   "source": [
    "### CREATE APP FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bae5865-f2be-42e8-8dab-d35bdfee01c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"templates\",exist_ok=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ed08f4-34b4-4fdc-bee4-cc2b0c9527b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting templates/predict_form.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile templates/predict_form.html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Trip Duration Predictor</title>\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
    "    <style>\n",
    "        :root { --primary:#007BFF; }\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            background: #f4f6f9;\n",
    "            padding: 20px;\n",
    "        }\n",
    "        .container {\n",
    "            background: #fff;\n",
    "            padding: 30px;\n",
    "            border-radius: 12px;\n",
    "            max-width: 560px;\n",
    "            margin: auto;\n",
    "            box-shadow: 0 10px 24px rgba(0,0,0,0.06);\n",
    "        }\n",
    "        .grid { display: grid; grid-template-columns: 1fr 1fr; gap: 16px; }\n",
    "        .grid .full { grid-column: 1 / -1; }\n",
    "        label { display:block; font-weight:600; margin-bottom:6px; }\n",
    "        input[type=\"number\"] {\n",
    "            width: 100%;\n",
    "            padding: 10px 12px;\n",
    "            border: 1px solid #d6dae1;\n",
    "            border-radius: 8px;\n",
    "            background:#fff;\n",
    "        }\n",
    "        button {\n",
    "            background: var(--primary);\n",
    "            color: #fff;\n",
    "            border: none;\n",
    "            padding: 12px 18px;\n",
    "            border-radius: 8px;\n",
    "            cursor: pointer;\n",
    "            font-weight: 600;\n",
    "        }\n",
    "        button:hover { background: #0056b3; }\n",
    "        .result {\n",
    "            margin-top: 20px;\n",
    "            background: #e6f7ff;\n",
    "            padding: 12px;\n",
    "            border-left: 4px solid var(--primary);\n",
    "            border-radius: 6px;\n",
    "        }\n",
    "        .subtitle { color:#566; margin-top:-6px; margin-bottom:18px; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h2>Trip Duration Predictor</h2>\n",
    "        <p class=\"subtitle\">Enter ride details and submit to get the predicted duration (minutes).</p>\n",
    "\n",
    "        <form method=\"post\">\n",
    "            <div class=\"grid\">\n",
    "                <div>\n",
    "                    <label for=\"passenger_count\">Passenger Count</label>\n",
    "                    <input type=\"number\" id=\"passenger_count\" name=\"passenger_count\" step=\"1\" min=\"0\" required placeholder=\"e.g., 1\">\n",
    "                </div>\n",
    "\n",
    "                <div>\n",
    "                    <label for=\"trip_distance\">Trip Distance (miles)</label>\n",
    "                    <input type=\"number\" id=\"trip_distance\" name=\"trip_distance\" step=\"0.01\" min=\"0\" required placeholder=\"e.g., 4.12\">\n",
    "                </div>\n",
    "\n",
    "                <div>\n",
    "                    <label for=\"fare_amount\">Fare Amount ($)</label>\n",
    "                    <input type=\"number\" id=\"fare_amount\" name=\"fare_amount\" step=\"0.01\" required placeholder=\"e.g., 21.20\">\n",
    "                </div>\n",
    "\n",
    "                <div>\n",
    "                    <label for=\"total_amount\">Total Amount ($)</label>\n",
    "                    <input type=\"number\" id=\"total_amount\" name=\"total_amount\" step=\"0.01\" required placeholder=\"e.g., 36.77\">\n",
    "                </div>\n",
    "\n",
    "                <div>\n",
    "                    <label for=\"PULocationID\">PU Location ID</label>\n",
    "                    <input type=\"number\" id=\"PULocationID\" name=\"PULocationID\" step=\"1\" min=\"0\" required placeholder=\"e.g., 171\">\n",
    "                </div>\n",
    "\n",
    "                <div>\n",
    "                    <label for=\"DOLocationID\">DO Location ID</label>\n",
    "                    <input type=\"number\" id=\"DOLocationID\" name=\"DOLocationID\" step=\"1\" min=\"0\" required placeholder=\"e.g., 73\">\n",
    "                </div>\n",
    "\n",
    "                <div class=\"full\" style=\"display:flex; justify-content:flex-end;\">\n",
    "                    <button type=\"submit\">Predict Duration</button>\n",
    "                </div>\n",
    "            </div>\n",
    "        </form>\n",
    "\n",
    "        {% if result %}\n",
    "        <div class=\"result\">\n",
    "            <strong>Prediction:</strong> {{ result }} minutes\n",
    "        </div>\n",
    "        {% endif %}\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6fd0ff3-bddc-47bb-bdcf-146ff1723a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Request, Form\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import uvicorn\n",
    "import logging\n",
    "import warnings\n",
    "from typing import List\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Suppress category_encoders warning\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='category_encoders')\n",
    "\n",
    "# Logging config\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Trip Duration Prediction API\",\n",
    "    description=\"API and Web UI for predicting NYC taxi trip duration using XGBoost\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Templates setup\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "# Globals\n",
    "preprocessor = None\n",
    "model = None\n",
    "\n",
    "# Input schema\n",
    "class RideData(BaseModel):\n",
    "    passenger_count: float\n",
    "    trip_distance: float\n",
    "    fare_amount: float\n",
    "    total_amount: float\n",
    "    PULocationID: int\n",
    "    DOLocationID: int\n",
    "    \n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"passenger_count\":1.0,\n",
    "                \"trip_distance\": 4.12,\n",
    "                \"fare_amount\":21.20,\n",
    "                \"total_amount\":36.77,\n",
    "                \"PULocationID\": 171,\n",
    "                \"DOLocationID\": 73,\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Output schema\n",
    "class PredictionResponse(BaseModel):\n",
    "    predicted_duration: float\n",
    "    status: str\n",
    "    message: str\n",
    "\n",
    "# Load preprocessor\n",
    "def load_preprocessor(path: str):\n",
    "    try:\n",
    "        preprocessor = joblib.load(path)\n",
    "        logger.info(\"Preprocessor loaded.\")\n",
    "        return preprocessor\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading preprocessor: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load model\n",
    "def load_model(path: str):\n",
    "    try:\n",
    "        model = XGBRegressor()\n",
    "        model.load_model(path) \n",
    "        logger.info(\"Model loaded.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Prediction logic\n",
    "def predict_duration(preprocessor, model, ride_df):\n",
    "    try:\n",
    "        X_processed = preprocessor.transform(ride_df)\n",
    "        prediction = model.predict(X_processed)\n",
    "        return float(prediction[0])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load on startup\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    global preprocessor, model\n",
    "    preprocessor = load_preprocessor(\"preprocessing.pkl\")\n",
    "    model = load_model(\"my_model.ubj\")\n",
    "\n",
    "# Health check\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Trip Duration Prediction API is running\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"preprocessor_loaded\": preprocessor is not None,\n",
    "        \"model_loaded\": model is not None\n",
    "    }\n",
    "\n",
    "# Single prediction\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(ride_data: RideData):\n",
    "    if preprocessor is None or model is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Models not loaded.\")\n",
    "    try:\n",
    "        df = pd.DataFrame([ride_data.dict()])\n",
    "        duration = predict_duration(preprocessor, model, df)\n",
    "        return PredictionResponse(\n",
    "            predicted_duration=duration,\n",
    "            status=\"success\",\n",
    "            message=\"Prediction completed successfully\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "# Batch prediction\n",
    "@app.post(\"/predict_batch\")\n",
    "async def predict_batch(rides: List[RideData]):\n",
    "    if preprocessor is None or model is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Models not loaded.\")\n",
    "    try:\n",
    "        results = []\n",
    "        for ride in rides:\n",
    "            df = pd.DataFrame([ride.dict()])\n",
    "            duration = predict_duration(preprocessor, model, df)\n",
    "            results.append(duration)\n",
    "        return {\n",
    "            \"predictions\": results,\n",
    "            \"status\": \"success\",\n",
    "            \"message\": f\"Predicted durations for {len(rides)} rides\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Batch prediction failed: {str(e)}\")\n",
    "\n",
    "# Web form GET\n",
    "@app.get(\"/form\", response_class=HTMLResponse)\n",
    "async def form_get(request: Request):\n",
    "    return templates.TemplateResponse(\"predict_form.html\", {\"request\": request, \"result\": None})\n",
    "\n",
    "# Web form POST\n",
    "# Web form POST\n",
    "@app.post(\"/form\", response_class=HTMLResponse)\n",
    "async def form_post(\n",
    "    request: Request,\n",
    "    passenger_count: float = Form(...),\n",
    "    trip_distance: float = Form(...),\n",
    "    fare_amount: float = Form(...),\n",
    "    total_amount: float = Form(...),\n",
    "    PULocationID: int = Form(...),\n",
    "    DOLocationID: int = Form(...),\n",
    "):\n",
    "    try:\n",
    "        if preprocessor is None or model is None:\n",
    "            raise HTTPException(status_code=500, detail=\"Model not loaded.\")\n",
    "        # Build the full feature set expected by the preprocessor/model\n",
    "        df = pd.DataFrame([{\n",
    "            \"passenger_count\": float(passenger_count),\n",
    "            \"trip_distance\": float(trip_distance),\n",
    "            \"fare_amount\": float(fare_amount),\n",
    "            \"total_amount\": float(total_amount),\n",
    "            \"PULocationID\": int(PULocationID),\n",
    "            \"DOLocationID\": int(DOLocationID),\n",
    "        }])\n",
    "        result = predict_duration(preprocessor, model, df)\n",
    "        return templates.TemplateResponse(\n",
    "            \"predict_form.html\",\n",
    "            {\"request\": request, \"result\": f\"{result:.2f}\"}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Form prediction error: {e}\")\n",
    "        return templates.TemplateResponse(\n",
    "            \"predict_form.html\",\n",
    "            {\"request\": request, \"result\": \"Error during prediction\"}\n",
    "        )\n",
    "\n",
    "\n",
    "# Run server\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=9696, reload=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb3164-a34c-4579-95e6-2e5f218573c1",
   "metadata": {},
   "source": [
    "### DOCKER FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f896dfc1-b029-4bb5-9463-cedc1b29a076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "##base image\n",
    "FROM python:3.12-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies and clean up in one layer\n",
    "RUN apt-get update -y && \\\n",
    "    apt-get install -y --no-install-recommends curl && \\\n",
    "    apt-get clean && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements first for better layer caching\n",
    "COPY requirements.txt /app/\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --upgrade pip && \\\n",
    "    pip install --no-cache-dir -r requirements.txt && \\\n",
    "    pip install --no-cache-dir awscli\n",
    "\n",
    "# Copy application files\n",
    "COPY . /app\n",
    "\n",
    "# Create non-root user for security\n",
    "RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app\n",
    "USER appuser\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 9696\n",
    "\n",
    "# Run the app\n",
    "ENTRYPOINT [\"uvicorn\", \"app:app\", \"--host=0.0.0.0\", \"--port=9696\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf84bc-463f-4486-a63c-6897aa830ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685f2bf-bfbd-4240-81e8-7f37cc762cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
