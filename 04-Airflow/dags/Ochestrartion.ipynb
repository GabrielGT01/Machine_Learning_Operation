{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f759fa97-103e-4db4-beec-9266bd4845a3",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Docker Compose Volume Configuration for Airflow\n",
    "\n",
    "To ensure the `data/` folder is accessible inside your Airflow containers (e.g., for downloading and storing Parquet files), add the following volume mapping `- ${AIRFLOW_PROJ_DIR:-.}/data:/opt/airflow/data` under the `volumes:` section in your `docker-compose.yaml`:\n",
    "\n",
    "```yaml\n",
    "volumes:\n",
    "  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n",
    "  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n",
    "  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n",
    "  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n",
    "  - ${AIRFLOW_PROJ_DIR:-.}/data:/opt/airflow/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27173b-97d7-488a-abe3-a77f2be2cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7b0a3d-00c3-4606-894f-89a614b07774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_script.py\n",
    "\n",
    "from datetime import datetime\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.sensors.base import PokeReturnValue\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Parameters tht can be changed\n",
    "YEAR = 2025\n",
    "MONTH = 5  \n",
    "\n",
    "@dag(\n",
    "    dag_id=\"data_engineering\",\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule=None,\n",
    "    catchup=False,\n",
    "    tags=[\"nyc-taxi\"]\n",
    ")\n",
    "def data_engineering():\n",
    "    \n",
    "    @task.sensor(poke_interval=30, timeout=300)\n",
    "    def is_api_available() -> PokeReturnValue:\n",
    "        \"\"\"\n",
    "        checks if the link still works\n",
    "        \"\"\"\n",
    "        url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=15)\n",
    "            print(f\"Status: {resp.status_code}\")\n",
    "            return PokeReturnValue(is_done=resp.ok)\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            return PokeReturnValue(is_done=False)\n",
    "    \n",
    "    @task\n",
    "    def download_green_taxi_data(year: int, month: int, output_dir: str = \"/opt/airflow/data\") -> str:\n",
    "        \"\"\"\n",
    "        Download the Green Taxi trip record Parquet file for the given year and month.\n",
    "        Returns the local destination path.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n",
    "        month_str = f\"{month:02d}\"\n",
    "        filename = f\"green_tripdata_{year}-{month_str}.parquet\"\n",
    "        url = f\"{base_url}/{filename}\"\n",
    "        dest_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        print(f\"Downloading {url} -> {dest_path}\")\n",
    "        \n",
    "        with requests.get(url, stream=True, timeout=60) as response:\n",
    "            response.raise_for_status()\n",
    "            with open(dest_path, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        \n",
    "        print(f\"Saved to {dest_path}\")\n",
    "        return dest_path\n",
    "    \n",
    "    @task\n",
    "    def feature_selection(link: str) -> str:\n",
    "        \"\"\"\n",
    "        Load and preprocess test data from parquet file\n",
    "        \n",
    "        Parameters:\n",
    "        link (str): Path to parquet file\n",
    "        \n",
    "        Returns:\n",
    "        str: Path to cleaned parquet file with the four columns required for training data\n",
    "        \"\"\"\n",
    "        data = pd.read_parquet(link)\n",
    "        data = data[['lpep_pickup_datetime', 'lpep_dropoff_datetime', \n",
    "                     'PULocationID', 'DOLocationID', 'trip_distance']]\n",
    "        \n",
    "        # Calculate trip duration in minutes\n",
    "        data['duration'] = (\n",
    "            data['lpep_dropoff_datetime'] - data['lpep_pickup_datetime']\n",
    "        ).dt.total_seconds() / 60\n",
    "        \n",
    "        # Select final columns\n",
    "        data = data[['PULocationID', 'DOLocationID', 'trip_distance', 'duration']]\n",
    "        \n",
    "        # Remove outliers - filter duration and distance\n",
    "        data = data[(data['duration'] >= 1) & (data['duration'] <= 62)]\n",
    "        data = data[(data['trip_distance'] >= 1) & (data['trip_distance'] <= 20)]\n",
    "        \n",
    "        # Convert location IDs to categorical data\n",
    "        data[['PULocationID', 'DOLocationID']] = (\n",
    "            data[['PULocationID', 'DOLocationID']].astype('str'))\n",
    "        \n",
    "        output_path = \"/opt/airflow/data/green_taxi_cleaned.parquet\"\n",
    "        # Save as Parquet\n",
    "        data.to_parquet(output_path, engine=\"pyarrow\", index=False)\n",
    "        \n",
    "        print(f\"Cleaned data saved to: {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    # Define tasks\n",
    "    api_ok = is_api_available()\n",
    "    # returns dest_path\n",
    "    downloaded = download_green_taxi_data.override(task_id=\"fetch_taxi_data\")(YEAR, MONTH)\n",
    "    \n",
    "    cleaned_data = feature_selection.override(task_id=\"clean_taxi_data\")(downloaded)\n",
    "    \n",
    "    \n",
    "    api_ok >> downloaded >> cleaned_data\n",
    "\n",
    "\n",
    "data_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4726d-7194-4af9-b353-634dd6936b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
